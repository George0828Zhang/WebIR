{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import mafan\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user mafan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_filename = 'inverted_file.json'\n",
    "url2content_name = 'url2content.json'\n",
    "url2title_name = 'url2titles.json'\n",
    "standlexi_name = 'extern/lexicon_mixed.json'\n",
    "\n",
    "doc2url_name = 'news_data_1/NC_1.csv'\n",
    "training_name = 'news_data_1/TD.csv'\n",
    "query_name = 'news_data_1/QS_1.csv'\n",
    "\n",
    "outcsv_name = 'out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 4\n",
    "MAXCAND = 300\n",
    "\n",
    "Okapi_k1 = 2.\n",
    "Okapi_b = 0.75\n",
    "Okapi_k3 = 500.\n",
    "\n",
    "\n",
    "IDF_epsilon = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "Ngram_weights = {}#{l:0. for l in range(1, 25)}\n",
    "Ngram_weights[1] = 1.\n",
    "Ngram_weights[2] = 1.04 #1.04: 5886\n",
    "Ngram_weights[3] = 0.4\n",
    "# Ngram_weights[4] = 0.4\n",
    "# Ngram_weights[5] = 1.\n",
    "MAXGRAM = max(Ngram_weights.keys())\n",
    "print(MAXGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, doc_id):\n",
    "        self.doc_id = ''\n",
    "        self.url = ''\n",
    "        self.tfreq = {}\n",
    "        self.id = doc_id\n",
    "        self.length = 0\n",
    "        self.normalized = False\n",
    "    def normalize(self, avgdl, IDF):\n",
    "        for t, f in self.tfreq.items():\n",
    "            TF = (Okapi_k1+1.)*f\n",
    "            dlen_norm = Okapi_k1*(1. - Okapi_b + Okapi_b * (self.length/avgdl)) + f\n",
    "            self.tfreq[t] = TF/dlen_norm * IDF[t]\n",
    "            assert(self.tfreq[t] > 0)\n",
    "        self.normalized = True\n",
    "    def update(self, term_id, tf):\n",
    "        if term_id in self.tfreq:\n",
    "            self.tfreq[term_id] += tf\n",
    "        else:\n",
    "            self.tfreq[term_id] = tf\n",
    "    def getFileSize(self, contents):\n",
    "        self.length = len(contents[self.url])\n",
    "        return self.length\n",
    "    def update_title(self, vocab):\n",
    "        r = requests.get(self.url)\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        if soup.title == None:\n",
    "            print('[warning]', self.doc_id, 'has no title!')\n",
    "        else:\n",
    "            title = str(soup.title).replace('<title>', '').replace('</title>', '')\n",
    "            self._process(title, vocab, 1.)\n",
    "    def _process(self, text, voc, weight):\n",
    "        size = len(text)\n",
    "        for start in range(size):\n",
    "            for ngram in range(1, MAXGRAM+1):\n",
    "                if Ngram_weights[ngram] <= 0 or ngram > size:\n",
    "                    break\n",
    "                    \n",
    "                end = start + ngram\n",
    "                word = text[start:end]\n",
    "                \n",
    "                if word in voc:\n",
    "                    index = voc[word]\n",
    "                    self.update(index, weight * Ngram_weights[ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf8len(s):\n",
    "    return len(s.encode('utf-8'))\n",
    "def getngram(w):\n",
    "    # check if it's chinese\n",
    "    is_eng = mafan.text.contains_latin(word)\n",
    "    return 99 if is_eng else len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News_Index,News_URL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "urlcontents = json.load(open(url2content_name, 'r'))\n",
    "tfdocs = {}\n",
    "with open(doc2url_name, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        else:\n",
    "            fields = line.strip().split(',') # doc_id, url\n",
    "            doc = Document(i-1)\n",
    "            doc.doc_id = fields[0]\n",
    "            doc.url = fields[1]\n",
    "            doc.length = utf8len(urlcontents[fields[1]])\n",
    "            tfdocs[fields[0]] = doc\n",
    "DOC_SZ = len(tfdocs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = json.load(open(inverted_filename, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186378\n"
     ]
    }
   ],
   "source": [
    "# load vocab\n",
    "make_vocab = True\n",
    "vocab_name = 'vocab.json'\n",
    "\n",
    "if make_vocab:\n",
    "    vocab = {}\n",
    "    IDF = {}\n",
    "    for (word, voc) in inverted.items():\n",
    "        ngram = getngram(word)\n",
    "        if ngram not in Ngram_weights:\n",
    "            continue\n",
    "        i = len(vocab)\n",
    "        vocab[word] = i\n",
    "#         idf = voc['idf']\n",
    "#         N = DOC_SZ / idf\n",
    "#         assert N <= DOC_SZ\n",
    "#         IDF[i] = max(IDF_epsilon, math.log((DOC_SZ - N + 0.5)/(N + 0.5)))\n",
    "    json.dump(vocab, open(vocab_name, 'w'))\n",
    "else:\n",
    "    vocab = json.load(open(vocab_name, 'r'))\n",
    "    IDF = {}\n",
    "\n",
    "VOC_SZ = len(vocab)\n",
    "print(VOC_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bec708316146168942c1974bbabe05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=217118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load raw tf\n",
    "for i, (word, voc) in tqdm(enumerate(inverted.items()), total=len(inverted)):\n",
    "    ngram = getngram(word)\n",
    "    if word not in vocab:\n",
    "        continue\n",
    "        \n",
    "    idf = voc['idf']\n",
    "    N = DOC_SZ / idf\n",
    "    assert N <= DOC_SZ\n",
    "    IDF[vocab[word]] = max(IDF_epsilon, math.log((DOC_SZ - N + 0.5)/(N + 0.5)))\n",
    "    \n",
    "    term_id = vocab[word]\n",
    "    for pair in voc['docs']:\n",
    "        for docname, tf in pair.items():           \n",
    "            tfdocs[docname].update(term_id, tf*Ngram_weights[ngram]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639e7a08ab904d04bdac08911c821c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "urltitles = json.load(open(url2title_name, 'r'))\n",
    "for d in tqdm(tfdocs.values()):\n",
    "    title = urltitles[d.url].strip()\n",
    "    if title == '':\n",
    "        #print('[warning]', d.doc_id, 'has no title !')\n",
    "        pass\n",
    "    else:\n",
    "        d._process(title, vocab, 1.)\n",
    "        d.length += utf8len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e393273d57f482c828d23dafa795438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2177.24542\n"
     ]
    }
   ],
   "source": [
    "# normalize docs\n",
    "avgdl = 0.\n",
    "for d in tfdocs.values():\n",
    "    avgdl += d.length\n",
    "avgdl /= DOC_SZ\n",
    "\n",
    "for d in tqdm(tfdocs.values()):\n",
    "    d.normalize(avgdl, IDF)\n",
    "for d in tfdocs.values():\n",
    "    assert d.normalized\n",
    "print(avgdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_a = 0.75\n",
    "R_b = 0.15\n",
    "R_c = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, qid, text, voc):\n",
    "        self.qid = qid\n",
    "        self.vec = {}\n",
    "        self.dim = len(voc)\n",
    "        self.id = int(qid[-2:])\n",
    "        self.length = 0\n",
    "        self.text = text\n",
    "        \n",
    "        self._process(text, voc, 1.)\n",
    "        self.normalize()\n",
    "    def match(self, doc_freq):\n",
    "        out = 0.\n",
    "        for t, f in doc_freq.items():\n",
    "            if t in self.vec:\n",
    "                out += self.vec[t] * f\n",
    "        return out\n",
    "    def match_2(self, doc_freq):\n",
    "        out = 0.\n",
    "        for t, f in self.vec.items():\n",
    "            if t in doc_freq:\n",
    "                out += f * doc_freq[t]\n",
    "        return out\n",
    "    def normalize(self):\n",
    "        for t, f in self.vec.items():\n",
    "            self.vec[t] = (Okapi_k3+1.)*self.vec[t] / (Okapi_k3+self.vec[t])\n",
    "        \n",
    "    def _process(self, text, voc, weight):\n",
    "        size = len(text)\n",
    "        for start in range(size):\n",
    "            for ngram in range(1, MAXGRAM+1):\n",
    "                if Ngram_weights[ngram] <= 0 or ngram > size:\n",
    "                    break\n",
    "                    \n",
    "                end = start + ngram\n",
    "                word = text[start:end]\n",
    "                \n",
    "                if word in voc:\n",
    "                    index = voc[word]\n",
    "                    prev = self.vec[index] if index in self.vec else 0\n",
    "                    self.vec[index] = prev + weight * Ngram_weights[ngram]\n",
    "   \n",
    "    def feedback(self, rel, irrel, tfdocs):\n",
    "        pos = {}\n",
    "        neg = {}\n",
    "        dim = self.dim\n",
    "\n",
    "        for i in rel:        \n",
    "            doc = tfdocs[i]\n",
    "            for t, f in doc.tfreq.items():\n",
    "                if t not in pos:\n",
    "                    pos[t] = 0\n",
    "                pos[t] += f / len(rel)\n",
    "\n",
    "        for i in irrel:\n",
    "            doc = tfdocs[i]\n",
    "            for t, f in doc.tfreq.items():\n",
    "                if t not in neg:\n",
    "                    neg[t] = 0\n",
    "                neg[t] += f / len(irrel)\n",
    "\n",
    "        for t in range(dim):\n",
    "            a = self.vec[t] if t in self.vec else 0\n",
    "            b = pos[t] if t in pos else 0\n",
    "            c = neg[t] if t in neg else 0\n",
    "\n",
    "            tmp = R_a * a + R_b * b - R_c * c\n",
    "            if tmp > 1e-4:\n",
    "                self.vec[t] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "standlexicon = json.load(open(standlexi_name, 'r'))\n",
    "standlexicon['negative'] = [w for w in standlexicon['negative'] if w in vocab]\n",
    "standlexicon['positive'] = [w for w in standlexicon['positive'] if w in vocab]\n",
    "# print(standlexicon)\n",
    "\n",
    "def adjust(q):\n",
    "    qneg = ['反對',  '拒絕',  '不應',  '錯誤',  '不可', '不贊同', '不贊成', '不合理',\n",
    "            '不對',  '不支持', '不同意', '不應該', '不正確','不可以','不合法' ]\n",
    "    qpos = ['支持', '同意', '應該', '正確', '可以', '贊同', '贊成', '合理', '合法', '不反對', '對的', '應', '可', '有']\n",
    "    \n",
    "    stand = None\n",
    "    key = None\n",
    "    for s in qneg:\n",
    "        if s in q.text:\n",
    "            print(q.text, '[neg]')\n",
    "            stand = 'neg'\n",
    "            key = s\n",
    "            break\n",
    "    if stand == None:\n",
    "        for s in qpos:\n",
    "            if s in q.text:\n",
    "                print(q.text, '[pos]')\n",
    "                stand = 'pos'\n",
    "                key = s\n",
    "                break\n",
    "    \n",
    "    if stand == 'neg':\n",
    "        if key in vocab:\n",
    "            index = vocab[key]\n",
    "            q.vec[index] *= 1.2\n",
    "        for w in standlexicon['negative']:\n",
    "            if w != key:\n",
    "                index = vocab[w]            \n",
    "                q.vec[index] = 0.33\n",
    "    elif stand == 'pos':\n",
    "        if key in vocab:\n",
    "            index = vocab[key]\n",
    "            q.vec[index] *= 1.2\n",
    "        for w in standlexicon['positive']:\n",
    "            if w != key:\n",
    "                index = vocab[w]            \n",
    "                q.vec[index] = 0.33\n",
    "    else:\n",
    "        print(q.text, '[none]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query,News_Index,Relevance\n",
      "\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "if not test:\n",
    "    num_train = 20\n",
    "    train_scores = {}\n",
    "    train_qlist = []\n",
    "    with open(training_name, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                print(line)\n",
    "            else:\n",
    "                fields = line.strip().split(',') # q_id, text\n",
    "                q, d, score = fields\n",
    "\n",
    "                score = int(score)\n",
    "\n",
    "                if q not in train_scores:\n",
    "                    train_scores[q] = {d:score}                \n",
    "                    train_qlist.append(Query(\"train_{:03d}\".format(len(train_qlist)), q, vocab))\n",
    "                else:\n",
    "                    train_scores[q][d] = score \n",
    "    # print(\"done\")\n",
    "    train_qlist = train_qlist[:num_train]\n",
    "    print(len(train_qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "支持陳前總統保外就醫 [pos]\n",
      "0.5757201239264498\n",
      "年金改革應取消或應調降軍公教月退之優存利率十八趴 [pos]\n",
      "0.5844330101457207\n",
      "同意動物實驗 [pos]\n",
      "0.6701215770637953\n",
      "油價應該凍漲或緩漲 [pos]\n",
      "0.5399031567289836\n",
      "反對旺旺中時併購中嘉 [neg]\n",
      "0.5990541903993526\n",
      "另立專法保障同婚是正確的 [pos]\n",
      "0.6385719759009506\n",
      "反對無圍牆校園 [neg]\n",
      "0.6210006158369632\n",
      "國際賽事會場內應該可以持中華民國國旗 [pos]\n",
      "0.7040066921611015\n",
      "贊同課綱微調 [pos]\n",
      "0.6973155009736804\n",
      "贊成流浪動物零撲殺 [pos]\n",
      "0.7895936440196538\n",
      "核四應該啟用 [pos]\n",
      "0.507705945617895\n",
      "贊成文林苑都更案可依法拆除王家 [pos]\n",
      "0.8120092022009424\n",
      "十二年國教高中職「免學費補助」適用對象增加是不對的 [neg]\n",
      "0.5586355366193766\n",
      "堅決反對政府舉債發展前瞻建設計畫 [neg]\n",
      "0.8072300572594174\n",
      "遠雄大巨蛋工程應停工或拆除 [pos]\n",
      "0.7845433747537782\n",
      "支持正名「臺灣」參與國際運動賽事 [pos]\n",
      "0.5170271545043451\n",
      "拒絕公投通過門檻下修 [neg]\n",
      "0.3176032028368193\n",
      "應該提高酒駕罰責以有效遏制酒駕 [pos]\n"
     ]
    }
   ],
   "source": [
    "if not test:\n",
    "    mean = 0\n",
    "\n",
    "    for q in train_qlist:\n",
    "        scores = []            \n",
    "        for d in tfdocs.values():\n",
    "            scores.append((d.doc_id, q.match_2(d.tfreq)))\n",
    "            \n",
    "########### feedback\n",
    "        scores = sorted(scores, key=lambda x: -x[1])        \n",
    "        \n",
    "        adjust(q)        \n",
    "        q.feedback([d for d, s in scores[:MAXCAND]], [d for d, s in scores[-MAXCAND:]], tfdocs)\n",
    "        \n",
    "        \n",
    "        scores = []            \n",
    "        for d in tfdocs.values():\n",
    "            scores.append((d.doc_id, q.match(d.tfreq)))  \n",
    "########### end            \n",
    "\n",
    "        scores = sorted(scores, key=lambda x: -x[1])[:MAXCAND]\n",
    "\n",
    "        myrank = [train_scores[q.text][d] if d in train_scores[q.text] else 0 for d, s in scores]\n",
    "        perfrank = sorted(myrank)[::-1]\n",
    "\n",
    "        my_dcg = 0\n",
    "        for i,r in enumerate(myrank):\n",
    "            if i == 0:\n",
    "                my_dcg += r\n",
    "            else:\n",
    "                my_dcg += r/math.log(i+1, 2)\n",
    "\n",
    "        perf_dcg = 0\n",
    "        for i,r in enumerate(perfrank):\n",
    "            if i == 0:\n",
    "                perf_dcg += r\n",
    "            else:\n",
    "                perf_dcg += r/math.log(i+1, 2)\n",
    "\n",
    "        cur = my_dcg/perf_dcg\n",
    "        print(cur)\n",
    "        mean += cur\n",
    "\n",
    "    print(\"[NDCG]\", mean / len(train_qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query_Index,Query\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load queries\n",
    "if test:\n",
    "    qlist = []\n",
    "\n",
    "    with open(query_name, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                print(line)\n",
    "            else:\n",
    "                fields = line.strip().split(',') # q_id, text\n",
    "                qlist.append(Query(fields[0], fields[1], vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "通姦在刑法上應該除罪化 [pos]\n",
      "Query 2\n",
      "應該取消機車強制二段式左轉(待轉) [pos]\n",
      "Query 3\n",
      "支持博弈特區在台灣合法化 [pos]\n",
      "Query 4\n",
      "中華航空空服員罷工是合理的 [pos]\n",
      "Query 5\n",
      "性交易應該合法化 [pos]\n",
      "Query 6\n",
      "ECFA早收清單可（有）達到其預期成效 [pos]\n",
      "Query 7\n",
      "應該減免證所稅 [pos]\n",
      "Query 8\n",
      "贊成中油在觀塘興建第三天然氣接收站 [pos]\n",
      "Query 9\n",
      "支持中國學生納入健保 [pos]\n",
      "Query 10\n",
      "支持臺灣中小學（含高職、專科）服儀規定（含髮、襪、鞋）給予學生自主 [pos]\n",
      "Query 11\n",
      "不支持使用加密貨幣 [neg]\n",
      "Query 12\n",
      "不支持學雜費調漲 [neg]\n",
      "Query 13\n",
      "同意政府舉債發展前瞻建設計畫 [pos]\n",
      "Query 14\n",
      "支持電競列入體育競技 [pos]\n",
      "Query 15\n",
      "反對台鐵東移徵收案 [neg]\n",
      "Query 16\n",
      "支持陳前總統保外就醫 [pos]\n",
      "Query 17\n",
      "年金改革應取消或應調降軍公教月退之優存利率十八趴 [pos]\n",
      "Query 18\n",
      "同意動物實驗 [pos]\n",
      "Query 19\n",
      "油價應該凍漲或緩漲 [pos]\n",
      "Query 20\n",
      "反對旺旺中時併購中嘉 [neg]\n"
     ]
    }
   ],
   "source": [
    "if test:\n",
    "    with open(outcsv_name, 'w') as writer:\n",
    "        writer.write(\"Query_Index\")\n",
    "        for i in range(MAXCAND):\n",
    "            writer.write(\",Rank_{:03d}\".format(i+1))\n",
    "    #     writer.write(\"\\n\")\n",
    "\n",
    "\n",
    "        for j,q in enumerate(qlist):\n",
    "            print(\"Query {}\".format(j+1))\n",
    "            \n",
    "            scores = []\n",
    "            for d in tfdocs.values():\n",
    "                scores.append((d.doc_id, q.match_2(d.tfreq)))\n",
    "            \n",
    "########### feedback\n",
    "            scores = sorted(scores, key=lambda x: -x[1])\n",
    "    \n",
    "            adjust(q)            \n",
    "            q.feedback([d for d, s in scores[:MAXCAND]], [d for d, s in scores[-MAXCAND:]], tfdocs)\n",
    "\n",
    "            scores = []            \n",
    "            for d in tfdocs.values():\n",
    "                scores.append((d.doc_id, q.match(d.tfreq)))  \n",
    "########### end    \n",
    "\n",
    "            scores = sorted(scores, key=lambda x: -x[1])[:MAXCAND]\n",
    "            assert scores[0][1] > scores[-1][1]\n",
    "\n",
    "            writer.write('\\n'+q.qid)\n",
    "            for doc_id, s in scores:\n",
    "                writer.write(','+doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
