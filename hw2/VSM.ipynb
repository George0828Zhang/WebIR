{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_filename = 'inverted_file.json'\n",
    "url2content_name = 'url2content.json'\n",
    "url2title_name = 'url2titles.json'\n",
    "\n",
    "doc2url_name = 'news_data_1/NC_1.csv'\n",
    "training_name = 'news_data_1/TD.csv'\n",
    "query_name = 'news_data_1/QS_1.csv'\n",
    "\n",
    "outcsv_name = 'out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "num_threads = 4\n",
    "MAXCAND = 300\n",
    "\n",
    "Okapi_k1 = 2.\n",
    "Okapi_b = 0.75\n",
    "Okapi_k3 = 500.\n",
    "\n",
    "# R_a = 0.85\n",
    "# R_b = 0.05\n",
    "# R_c = 0.1\n",
    "\n",
    "IDF_epsilon = 1e-4\n",
    "Ngram_weights = {l:1. for l in range(1, 25)}\n",
    "MAXGRAM = max(Ngram_weights.keys())\n",
    "print(MAXGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngram_weights = {l:(1. - 0.04*(l-2)) for l in range(1, 25)} 0.49\n",
    "Ngram_weights = {l:0. for l in range(1, 25)}\n",
    "Ngram_weights[1] = 1.\n",
    "Ngram_weights[2] = 1.04 #1.04: 5886\n",
    "Ngram_weights[3] = 0.4\n",
    "# Ngram_weights[2] = 0.88\n",
    "# Ngram_weights[3] = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, doc_id):\n",
    "        self.doc_id = ''\n",
    "        self.url = ''\n",
    "        self.tfreq = {}\n",
    "        self.id = doc_id\n",
    "        self.length = 0\n",
    "        self.normalized = False\n",
    "    def normalize(self, avgdl, IDF):\n",
    "        for t, f in self.tfreq.items():\n",
    "            TF = (Okapi_k1+1.)*f\n",
    "            dlen_norm = Okapi_k1*(1. - Okapi_b + Okapi_b * (self.length/avgdl)) + f\n",
    "            self.tfreq[t] = TF/dlen_norm * IDF[t]\n",
    "            assert(self.tfreq[t] > 0)\n",
    "        self.normalized = True\n",
    "    def update(self, term_id, tf):\n",
    "        if term_id in self.tfreq:\n",
    "            self.tfreq[term_id] += tf\n",
    "        else:\n",
    "            self.tfreq[term_id] = tf\n",
    "    def getFileSize(self, contents):\n",
    "        self.length = len(contents[self.url])\n",
    "        return self.length\n",
    "    def update_title(self, vocab):\n",
    "        r = requests.get(self.url)\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        if soup.title == None:\n",
    "            print('[warning]', self.doc_id, 'has no title!')\n",
    "        else:\n",
    "            title = str(soup.title).replace('<title>', '').replace('</title>', '')\n",
    "            self._process(title, vocab, 1.)\n",
    "    def _process(self, text, voc, weight):\n",
    "        size = len(text)\n",
    "        for start in range(size):\n",
    "            for ngram in range(1, MAXGRAM+1):\n",
    "                if Ngram_weights[ngram] <= 0 or ngram > size:\n",
    "                    break\n",
    "                    \n",
    "                end = start + ngram\n",
    "                word = text[start:end]\n",
    "                \n",
    "                if word in voc:\n",
    "                    index = voc[word]\n",
    "                    self.update(index, weight * Ngram_weights[ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf8len(s):\n",
    "    return len(s.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News_Index,News_URL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "urlcontents = json.load(open(url2content_name, 'r'))\n",
    "tfdocs = {}\n",
    "with open(doc2url_name, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        else:\n",
    "            fields = line.strip().split(',') # doc_id, url\n",
    "            doc = Document(i-1)\n",
    "            doc.doc_id = fields[0]\n",
    "            doc.url = fields[1]\n",
    "            doc.length = utf8len(urlcontents[fields[1]])\n",
    "            tfdocs[fields[0]] = doc\n",
    "DOC_SZ = len(tfdocs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = json.load(open(inverted_filename, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190376\n"
     ]
    }
   ],
   "source": [
    "# load vocab\n",
    "vocab = {}\n",
    "IDF = {}\n",
    "for (word, voc) in inverted.items():\n",
    "    ngram = len(word)\n",
    "    if Ngram_weights[ngram] <= 0:\n",
    "        continue\n",
    "    i = len(vocab)\n",
    "    vocab[word] = i\n",
    "    idf = voc['idf']\n",
    "    N = DOC_SZ / idf\n",
    "    assert N <= DOC_SZ\n",
    "    IDF[i] = max(IDF_epsilon, math.log((DOC_SZ - N + 0.5)/(N + 0.5)))\n",
    "#     IDF[i] = max(IDF_epsilon, math.log(idf))\n",
    "    \n",
    "VOC_SZ = len(vocab)\n",
    "print(VOC_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883386b0a5ec402eb68263477cf90e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=217118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load raw tf\n",
    "for i, (word, voc) in tqdm(enumerate(inverted.items()), total=len(inverted)):\n",
    "    ngram = len(word)\n",
    "    if word not in vocab:\n",
    "        continue\n",
    "    term_id = vocab[word]\n",
    "    for pair in voc['docs']:\n",
    "        for docname, tf in pair.items():           \n",
    "            tfdocs[docname].update(term_id, tf*Ngram_weights[ngram]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f3c6f290d2433a932046c11dcb1900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "urltitles = json.load(open(url2title_name, 'r'))\n",
    "for d in tqdm(tfdocs.values()):\n",
    "    title = urltitles[d.url].strip()\n",
    "    if title == '':\n",
    "        #print('[warning]', d.doc_id, 'has no title !')\n",
    "        pass\n",
    "    else:\n",
    "        d._process(title, vocab, 1.)\n",
    "        d.length += utf8len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2177.24542\n"
     ]
    }
   ],
   "source": [
    "# normalize docs\n",
    "avgdl = 0.\n",
    "for d in tfdocs.values():\n",
    "    avgdl += d.length\n",
    "avgdl /= DOC_SZ\n",
    "\n",
    "for d in tfdocs.values():\n",
    "    d.normalize(avgdl, IDF)\n",
    "for d in tfdocs.values():\n",
    "    assert d.normalized\n",
    "print(avgdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_a = 0.75\n",
    "R_b = 0.15\n",
    "R_c = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, qid, text, voc):\n",
    "        self.qid = qid\n",
    "        self.vec = {}\n",
    "        self.dim = len(voc)\n",
    "        self.id = int(qid[-2:])\n",
    "        self.length = 0\n",
    "        self.text = text\n",
    "        \n",
    "        self._process(text, voc, 1.)\n",
    "        self.normalize()\n",
    "    def match(self, doc_freq):\n",
    "        out = 0.\n",
    "        for t, f in doc_freq.items():\n",
    "            if t in self.vec:\n",
    "                out += self.vec[t] * f\n",
    "        return out\n",
    "    def match_2(self, doc_freq):\n",
    "        out = 0.\n",
    "        for t, f in self.vec.items():\n",
    "            if t in doc_freq:\n",
    "                out += f * doc_freq[t]\n",
    "        return out\n",
    "    def normalize(self):\n",
    "        for t, f in self.vec.items():\n",
    "            self.vec[t] = (Okapi_k3+1.)*self.vec[t] / (Okapi_k3+self.vec[t])\n",
    "        \n",
    "    def _process(self, text, voc, weight):\n",
    "        size = len(text)\n",
    "        for start in range(size):\n",
    "            for ngram in range(1, MAXGRAM+1):\n",
    "                if Ngram_weights[ngram] <= 0 or ngram > size:\n",
    "                    break\n",
    "                    \n",
    "                end = start + ngram\n",
    "                word = text[start:end]\n",
    "                \n",
    "                if word in voc:\n",
    "                    index = voc[word]\n",
    "                    prev = self.vec[index] if index in self.vec else 0\n",
    "                    self.vec[index] = prev + weight * Ngram_weights[ngram]\n",
    "   \n",
    "    def feedback(self, rel, irrel, tfdocs):\n",
    "        pos = {}\n",
    "        neg = {}\n",
    "        dim = self.dim\n",
    "\n",
    "        for i in rel:        \n",
    "            doc = tfdocs[i]\n",
    "            for t, f in doc.tfreq.items():\n",
    "                if t not in pos:\n",
    "                    pos[t] = 0\n",
    "                pos[t] += f / len(rel)\n",
    "\n",
    "        for i in irrel:\n",
    "            doc = tfdocs[i]\n",
    "            for t, f in doc.tfreq.items():\n",
    "                if t not in neg:\n",
    "                    neg[t] = 0\n",
    "                neg[t] += f / len(irrel)\n",
    "\n",
    "        for t in range(dim):\n",
    "            a = self.vec[t] if t in self.vec else 0\n",
    "            b = pos[t] if t in pos else 0\n",
    "            c = neg[t] if t in neg else 0\n",
    "\n",
    "            tmp = R_a * a + R_b * b - R_c * c\n",
    "            if tmp > 1e-4:\n",
    "                self.vec[t] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query,News_Index,Relevance\n",
      "\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "if not test:\n",
    "    num_train = 20\n",
    "    train_scores = {}\n",
    "    train_qlist = []\n",
    "    with open(training_name, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                print(line)\n",
    "            else:\n",
    "                fields = line.strip().split(',') # q_id, text\n",
    "                q, d, score = fields\n",
    "\n",
    "                score = int(score)\n",
    "\n",
    "                if q not in train_scores:\n",
    "                    train_scores[q] = {d:score}                \n",
    "                    train_qlist.append(Query(\"train_{:03d}\".format(len(train_qlist)), q, vocab))\n",
    "                else:\n",
    "                    train_scores[q][d] = score \n",
    "    # print(\"done\")\n",
    "    train_qlist = train_qlist[:num_train]\n",
    "    print(len(train_qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5321209105892122\n",
      "0.5636042046580734\n",
      "0.5476786125680425\n",
      "0.5229674122483885\n",
      "0.5802183766594967\n",
      "0.6618616985606891\n",
      "0.6321486392685015\n",
      "0.7105221146899685\n",
      "0.6593882926125051\n",
      "0.8279933977506758\n",
      "0.5850924574576233\n",
      "0.8448235086059627\n",
      "0.5713745567540826\n",
      "0.7623995901990883\n",
      "0.7857883804572791\n",
      "0.5274236205846091\n",
      "0.2884595707446423\n",
      "0.9071764005499712\n",
      "0.6480561430460704\n",
      "0.3773925921994312\n",
      "[NDCG] 0.6268245240102156\n"
     ]
    }
   ],
   "source": [
    "if not test:\n",
    "    mean = 0\n",
    "\n",
    "    for q in train_qlist:\n",
    "        scores = []            \n",
    "        for d in tfdocs.values():\n",
    "            scores.append((d.doc_id, q.match_2(d.tfreq)))\n",
    "            \n",
    "########### feedback\n",
    "        scores = sorted(scores, key=lambda x: -x[1])\n",
    "        \n",
    "        q.feedback([d for d, s in scores[:MAXCAND]], [d for d, s in scores[-MAXCAND:]], tfdocs)\n",
    "        \n",
    "        scores = []            \n",
    "        for d in tfdocs.values():\n",
    "            scores.append((d.doc_id, q.match(d.tfreq)))  \n",
    "########### end            \n",
    "\n",
    "        scores = sorted(scores, key=lambda x: -x[1])[:MAXCAND]\n",
    "\n",
    "        myrank = [train_scores[q.text][d] if d in train_scores[q.text] else 0 for d, s in scores]\n",
    "        perfrank = sorted(myrank)[::-1]\n",
    "\n",
    "        my_dcg = 0\n",
    "        for i,r in enumerate(myrank):\n",
    "            if i == 0:\n",
    "                my_dcg += r\n",
    "            else:\n",
    "                my_dcg += r/math.log(i+1, 2)\n",
    "\n",
    "        perf_dcg = 0\n",
    "        for i,r in enumerate(perfrank):\n",
    "            if i == 0:\n",
    "                perf_dcg += r\n",
    "            else:\n",
    "                perf_dcg += r/math.log(i+1, 2)\n",
    "\n",
    "        cur = my_dcg/perf_dcg\n",
    "        print(cur)\n",
    "        mean += cur\n",
    "\n",
    "    print(\"[NDCG]\", mean / len(train_qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query_Index,Query\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load queries\n",
    "if test:\n",
    "    qlist = []\n",
    "\n",
    "    with open(query_name, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                print(line)\n",
    "            else:\n",
    "                fields = line.strip().split(',') # q_id, text\n",
    "                qlist.append(Query(fields[0], fields[1], vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "Query 2\n",
      "Query 3\n",
      "Query 4\n",
      "Query 5\n",
      "Query 6\n",
      "Query 7\n",
      "Query 8\n",
      "Query 9\n",
      "Query 10\n",
      "Query 11\n",
      "Query 12\n",
      "Query 13\n",
      "Query 14\n",
      "Query 15\n",
      "Query 16\n",
      "Query 17\n",
      "Query 18\n",
      "Query 19\n",
      "Query 20\n"
     ]
    }
   ],
   "source": [
    "if test:\n",
    "    with open(outcsv_name, 'w') as writer:\n",
    "        writer.write(\"Query_Index\")\n",
    "        for i in range(MAXCAND):\n",
    "            writer.write(\",Rank_{:03d}\".format(i+1))\n",
    "    #     writer.write(\"\\n\")\n",
    "\n",
    "\n",
    "        for j,q in enumerate(qlist):\n",
    "            print(\"Query {}\".format(j+1))\n",
    "\n",
    "            scores = []\n",
    "\n",
    "#             def subtask(d):\n",
    "#                 return (d.doc_id, q.match(d.tfreq))\n",
    "\n",
    "#             with Pool(num_threads) as p:\n",
    "#                 chunksize = 1000\n",
    "#                 scores = list(p.imap_unordered(subtask, tfdocs, chunksize=chunksize))\n",
    "            \n",
    "            scores = []\n",
    "            for d in tfdocs.values():\n",
    "                scores.append((d.doc_id, q.match_2(d.tfreq)))\n",
    "            \n",
    "########### feedback\n",
    "            scores = sorted(scores, key=lambda x: -x[1])\n",
    "\n",
    "            q.feedback([d for d, s in scores[:MAXCAND]], [d for d, s in scores[-MAXCAND:]], tfdocs)\n",
    "\n",
    "            scores = []            \n",
    "            for d in tfdocs.values():\n",
    "                scores.append((d.doc_id, q.match(d.tfreq)))  \n",
    "########### end    \n",
    "\n",
    "            scores = sorted(scores, key=lambda x: -x[1])[:MAXCAND]\n",
    "            assert scores[0][1] > scores[-1][1]\n",
    "\n",
    "            writer.write('\\n'+q.qid)\n",
    "            for doc_id, s in scores:\n",
    "                writer.write(','+doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
