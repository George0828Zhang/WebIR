{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_filename = 'inverted_file.json'\n",
    "url2content_name = 'url2content.json'\n",
    "\n",
    "doc2url_name = 'news_data_1/NC_1.csv'\n",
    "training_name = 'news_data_1/TD.csv'\n",
    "query_name = 'news_data_1/QS_1.csv'\n",
    "\n",
    "outcsv_name = 'out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "num_threads = 4\n",
    "MAXCAND = 300\n",
    "Okapi_k1 = 2.0\n",
    "Okapi_b = 0.75\n",
    "Okapi_k3 = 500.\n",
    "IDF_epsilon = 1e-4\n",
    "Ngram_weights = {l:1. for l in range(1, 25)}\n",
    "MAXGRAM = max(Ngram_weights.keys())\n",
    "print(MAXGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngram_weights = {l:(1. - 0.04*(l-2)) for l in range(1, 25)} 0.49\n",
    "Ngram_weights = {l:0. for l in range(1, 25)}\n",
    "Ngram_weights[1] = 1.\n",
    "Ngram_weights[2] = 0.88\n",
    "Ngram_weights[3] = 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, doc_id):\n",
    "        self.doc_id = ''\n",
    "        self.url = ''\n",
    "        self.tid = []\n",
    "        self.freq = []\n",
    "        self.id = doc_id\n",
    "        self.length = 0\n",
    "        self.normalized = False\n",
    "    def normalize(self, avgdl, IDF):        \n",
    "        for j,t in enumerate(self.tid):\n",
    "            TF = (Okapi_k1+1.)*self.freq[j]\n",
    "            dlen_norm = Okapi_k1*(1. - Okapi_b + Okapi_b * (self.length/avgdl)) + self.freq[j]\n",
    "            self.freq[j] = TF/dlen_norm * IDF[t]\n",
    "            assert(self.freq[j] > 0)\n",
    "        self.normalized = True\n",
    "    def update(self, term_id, tf, is_uniq):\n",
    "        if not is_uniq:\n",
    "            for j, t in enumerate(self.tid):\n",
    "                if t == term_id:\n",
    "                    freq[j] += tf\n",
    "                    return\n",
    "        self.tid.append(term_id)\n",
    "        self.freq.append(tf)\n",
    "    def getFileSize(self, contents):\n",
    "        self.length = len(contents[self.url])\n",
    "        return self.length\n",
    "    def update_title(self, vocab):\n",
    "        pass\n",
    "    def _process():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, qid, text, voc):\n",
    "        self.qid = qid\n",
    "        self.vec = np.zeros(len(voc), dtype=np.float32)\n",
    "        self.dim = len(voc)\n",
    "        self.id = int(qid[-2:])\n",
    "        self.length = 0\n",
    "        self.text = text\n",
    "        \n",
    "        self._process(text, voc, 1.)\n",
    "        self.normalize()\n",
    "    def match(self, doc):\n",
    "        out = 0.\n",
    "        for i, t in enumerate(doc.tid):\n",
    "            out += self.vec[t] * doc.freq[i]\n",
    "        return out\n",
    "    def normalize(self):\n",
    "        for j in range(self.dim):\n",
    "            self.vec[j] = (Okapi_k3+1.)*self.vec[j] / (Okapi_k3+self.vec[j])\n",
    "    def _process(self, text, voc, weight):\n",
    "        size = len(text)\n",
    "        for start in range(size):\n",
    "            for ngram in range(1, MAXGRAM+1):\n",
    "                if Ngram_weights[ngram] <= 0 or ngram > size:\n",
    "                    break\n",
    "                    \n",
    "                end = start + ngram\n",
    "                word = text[start:end]\n",
    "                \n",
    "                if word in voc:\n",
    "                    index = voc[word]\n",
    "                    self.vec[index] += weight * Ngram_weights[ngram]\n",
    "                #else:\n",
    "                #    print('[debug]', word, 'not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News_Index,News_URL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "urlcontents = json.load(open(url2content_name, 'r'))\n",
    "tfdocs = []\n",
    "doc_ids = {}\n",
    "with open(doc2url_name, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        else:\n",
    "            fields = line.strip().split(',') # doc_id, url\n",
    "            doc = Document(i-1)\n",
    "            doc.doc_id = fields[0]\n",
    "            doc.url = fields[1]\n",
    "            doc.length = len(urlcontents[fields[1]])\n",
    "            doc_ids[fields[0]] = i-1\n",
    "            tfdocs.append(doc)\n",
    "DOC_SZ = len(tfdocs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = json.load(open(inverted_filename, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190376\n"
     ]
    }
   ],
   "source": [
    "# load vocab\n",
    "vocab = {}\n",
    "IDF = {}\n",
    "for (word, voc) in inverted.items():\n",
    "    ngram = len(word)\n",
    "    if Ngram_weights[ngram] <= 0:\n",
    "        continue\n",
    "    i = len(vocab)\n",
    "    vocab[word] = i\n",
    "    idf = voc['idf']\n",
    "    N = DOC_SZ / idf\n",
    "    assert N <= DOC_SZ\n",
    "    IDF[i] = max(IDF_epsilon, math.log((DOC_SZ - N + 0.5)/(N + 0.5)))\n",
    "    \n",
    "#     IDF[i] = voc['idf']\n",
    "# std::max(IDF_epsilon, std::log((DOC_SZ - N + 0.5)/(N + 0.5)))\n",
    "VOC_SZ = len(vocab)\n",
    "print(VOC_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a77a6971c94f738a0e9f8f4515eb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=217118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load raw tf\n",
    "for i, (word, voc) in tqdm(enumerate(inverted.items()), total=len(inverted)):\n",
    "    ngram = len(word)\n",
    "    if word not in vocab:\n",
    "        continue\n",
    "    term_id = vocab[word]\n",
    "    for pair in voc['docs']:\n",
    "        for docname, tf in pair.items():\n",
    "            file_id = doc_ids[docname]\n",
    "            tfdocs[file_id].update(term_id, tf*Ngram_weights[ngram], True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738.53705\n"
     ]
    }
   ],
   "source": [
    "# normalize docs\n",
    "avgdl = 0.\n",
    "for d in tfdocs:\n",
    "    avgdl += d.length\n",
    "avgdl /= DOC_SZ\n",
    "\n",
    "for d in tfdocs:\n",
    "    d.normalize(avgdl, IDF)\n",
    "for d in tfdocs:\n",
    "    assert d.normalized\n",
    "print(avgdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query,News_Index,Relevance\n",
      "\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "if not test:\n",
    "    num_train = 10\n",
    "    train_scores = {}\n",
    "    train_qlist = []\n",
    "    with open(training_name, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                print(line)\n",
    "            else:\n",
    "                fields = line.strip().split(',') # q_id, text\n",
    "                q, d, score = fields\n",
    "\n",
    "                score = int(score)\n",
    "\n",
    "                if q not in train_scores:\n",
    "                    train_scores[q] = {d:score}                \n",
    "                    train_qlist.append(Query(\"train_{:03d}\".format(len(train_qlist)), q, vocab))\n",
    "                else:\n",
    "                    train_scores[q][d] = score \n",
    "    # print(\"done\")\n",
    "    train_qlist = train_qlist[:num_train]\n",
    "    print(len(train_qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not test:\n",
    "    mean = 0\n",
    "\n",
    "    for q in train_qlist:\n",
    "        scores = []\n",
    "\n",
    "        def subtask(d):\n",
    "            return (d.doc_id, q.match(d))\n",
    "\n",
    "        with Pool(num_threads) as p:\n",
    "            chunksize = 500\n",
    "    #         scores = list(tqdm(p.imap(subtask, tfdocs, chunksize=chunksize), total=DOC_SZ))\n",
    "            scores = list(p.imap(subtask, tfdocs, chunksize=chunksize))\n",
    "\n",
    "        scores = sorted(scores, key=lambda x: -x[1])[:MAXCAND]\n",
    "        assert scores[0][1] > scores[-1][1]\n",
    "\n",
    "        myrank = [train_scores[q.text][d] if d in train_scores[q.text] else 0 for d, s in scores]\n",
    "        perfrank = sorted(myrank)[::-1]\n",
    "\n",
    "        my_dcg = 0\n",
    "        for i,r in enumerate(myrank):\n",
    "            if i == 0:\n",
    "                my_dcg += r\n",
    "            else:\n",
    "                my_dcg += r/math.log(i+1, 2)\n",
    "\n",
    "        perf_dcg = 0\n",
    "        for i,r in enumerate(perfrank):\n",
    "            if i == 0:\n",
    "                perf_dcg += r\n",
    "            else:\n",
    "                perf_dcg += r/math.log(i+1, 2)\n",
    "\n",
    "        cur = my_dcg/perf_dcg\n",
    "        print(cur)\n",
    "        mean += cur\n",
    "\n",
    "    print(\"[NDCG]\", mean / len(train_qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load queries\n",
    "if test:\n",
    "    qlist = []\n",
    "\n",
    "    with open(query_name, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                print(line)\n",
    "            else:\n",
    "                fields = line.strip().split(',') # q_id, text\n",
    "                qlist.append(Query(fields[0], fields[1], vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    outcsv_name = 'out.csv'\n",
    "    with open(outcsv_name, 'w') as writer:\n",
    "        writer.write(\"Query_Index\")\n",
    "        for i in range(MAXCAND):\n",
    "            writer.write(\",Rank_{:03d}\".format(i+1))\n",
    "    #     writer.write(\"\\n\")\n",
    "\n",
    "\n",
    "        for j,q in enumerate(qlist):\n",
    "            print(\"Query {}\".format(j+1))\n",
    "\n",
    "            scores = []\n",
    "\n",
    "            def subtask(d):\n",
    "                return (d.doc_id, q.match(d))\n",
    "            #for doc in tqdm(tfdocs):\n",
    "            #    scores.append((doc.doc_id, q.match(doc)))\n",
    "\n",
    "            with Pool(num_threads) as p:\n",
    "                chunksize = 500\n",
    "                #scores = list(tqdm(p.imap_unordered(subtask, tfdocs, chunksize=chunksize), total=DOC_SZ))\n",
    "                scores = list(tqdm(p.imap(subtask, tfdocs, chunksize=chunksize), total=DOC_SZ))\n",
    "\n",
    "\n",
    "            scores = sorted(scores, key=lambda x: -x[1])[:MAXCAND]\n",
    "            assert scores[0][1] > scores[-1][1]\n",
    "\n",
    "            writer.write('\\n'+q.qid)\n",
    "            for doc_id, s in scores:\n",
    "                writer.write(','+doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
