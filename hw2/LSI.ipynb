{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "from gensim import corpora, models, similarities\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_filename = 'inverted_file.json'\n",
    "url2content_name = 'url2content.json'\n",
    "url2title_name = 'url2titles.json'\n",
    "standlexi_name = 'extern/lexicon_mixed.json'\n",
    "stoplist_name = 'extern/stop_list.json'\n",
    "\n",
    "doc2url_name = 'news_data_1/NC_1.csv'\n",
    "training_name = 'news_data_1/TD.csv'\n",
    "query_name = 'news_data_1/QS_1.csv'\n",
    "\n",
    "outcsv_name = 'out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 16\n",
    "MAXCAND = 300\n",
    "DOCSIZE = 100000\n",
    "num_topics = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXGRAM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"loading\")\n",
    "inverted = json.load(open(inverted_filename, 'r'))\n",
    "stop_list = json.load(open(stoplist_name, 'r'))\n",
    "urlcontents = json.load(open(url2content_name, 'r'))\n",
    "urltitles = json.load(open(url2title_name, 'r'))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460e88acd6fa4c30b4698b8275fac68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News_Index,News_URL\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load text into corpera\n",
    "raw_documents = []\n",
    "doc_index = {}\n",
    "\n",
    "with open(doc2url_name, 'r') as f:\n",
    "    for i, line in tqdm(enumerate(f), total=DOCSIZE):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        else:\n",
    "            docname, url = line.strip().split(',') # doc_id, url\n",
    "            d_id = len(raw_documents)\n",
    "            doc_index[docname] = d_id\n",
    "            text = urlcontents[url]+urltitles[url]\n",
    "            raw_documents.append(text)\n",
    "\n",
    "index2doc = {a:b for b, a in doc_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/student/05/b05902064/.local/lib/python3.7/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp0w0eo5dg' -> '/tmp/jieba.cache'\n",
      "Loading model cost 1.512 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello', ' ', 'world', ' ', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# warmup\n",
    "list(jieba.cut(\"Hello world !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce35a817912940f88faa31063c6c575c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# use jieba to cut the sentences\n",
    "def taskseg(s):\n",
    "    it = jieba.cut(s)\n",
    "    return [w for w in it if w in inverted and w not in stop_list]\n",
    "\n",
    "with Pool(num_threads) as p:\n",
    "    chunksize = 100\n",
    "    raw_documents = list(tqdm(p.imap(taskseg, raw_documents, chunksize=chunksize), total=DOCSIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use inverted file to build the documents\n",
    "# raw_documents = []\n",
    "# doc_index = {}\n",
    "# doc2url = {}\n",
    "# with open(doc2url_name, 'r') as f:\n",
    "#     for i, line in tqdm(enumerate(f), total=DOCSIZE):\n",
    "#         if i == 0:\n",
    "#             print(line)\n",
    "#         else:\n",
    "#             docname, url = line.strip().split(',') # doc_id, url\n",
    "#             d_id = len(raw_documents)\n",
    "#             doc_index[docname] = d_id\n",
    "#             text = urlcontents[url]+urltitles[url]\n",
    "#             raw_documents.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary\n",
    "dictionary = corpora.Dictionary(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in raw_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_model = models.TfidfModel(corpus)\n",
    "# corpus_tfidf = tfidf_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "# corpus_lsi = lsi[corpus_tfidf] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "corpus_lsi = lsi[corpus_tfidf] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.111*\"台灣\" + 0.098*\"％\" + 0.088*\"（\" + 0.088*\"）\" + 0.088*\".\" + 0.087*\"年\" + 0.085*\"政府\" + 0.083*\"陳\" + 0.080*\"：\" + 0.080*\"馬\"'),\n",
       " (1,\n",
       "  '0.204*\".\" + 0.171*\"％\" + -0.149*\"公投\" + -0.149*\"課綱\" + 0.145*\"元\" + -0.137*\"黨\" + 0.134*\"市場\" + 0.130*\"億元\" + 0.127*\"台股\" + 0.122*\"美元\"'),\n",
       " (2,\n",
       "  '0.483*\"巨蛋\" + 0.435*\"遠雄\" + 0.333*\"市府\" + 0.182*\"柯文\" + -0.152*\"公投\" + 0.140*\"大\" + 0.125*\"停工\" + 0.123*\"北\" + -0.109*\"黨\" + 0.101*\"哲\"'),\n",
       " (3,\n",
       "  '0.439*\"課綱\" + 0.338*\"教育部\" + 0.245*\"學生\" + -0.220*\"公投\" + 0.151*\"高中\" + 0.146*\"微調\" + 0.131*\"教育\" + -0.119*\"巨蛋\" + -0.118*\"黨\" + 0.115*\"學校\"'),\n",
       " (4,\n",
       "  '-0.314*\"課綱\" + -0.214*\"教育部\" + -0.172*\"巨蛋\" + -0.163*\"遠雄\" + 0.153*\"水扁\" + 0.150*\"扁\" + -0.149*\"％\" + 0.145*\"保外\" + 0.142*\"陳\" + -0.127*\"公投\"'),\n",
       " (5,\n",
       "  '-0.373*\"Uber\" + 0.217*\"水扁\" + 0.212*\"扁\" + 0.210*\"中油\" + 0.207*\"保外\" + 0.204*\"元\" + 0.186*\"油價\" + 0.172*\"陳\" + 0.151*\"公升\" + 0.134*\"就醫\"'),\n",
       " (6,\n",
       "  '0.244*\"中油\" + 0.210*\"公投\" + -0.203*\"水扁\" + -0.199*\"課綱\" + -0.193*\"扁\" + -0.191*\"保外\" + 0.185*\"Uber\" + 0.183*\"油價\" + 0.180*\"元\" + 0.158*\"公升\"'),\n",
       " (7,\n",
       "  '-0.581*\"Uber\" + 0.208*\"公投\" + 0.181*\".\" + 0.152*\"婚姻\" + -0.143*\"水扁\" + -0.140*\"保外\" + -0.135*\"扁\" + 0.134*\"同性\" + -0.128*\"交通部\" + -0.127*\"中油\"'),\n",
       " (8,\n",
       "  '-0.229*\"中油\" + 0.221*\"Uber\" + -0.189*\"兩岸\" + -0.181*\"油價\" + 0.165*\"％\" + -0.155*\"台灣\" + 0.150*\"退休\" + 0.145*\"婚姻\" + -0.137*\"公升\" + 0.132*\"同性\"'),\n",
       " (9,\n",
       "  '0.322*\"Uber\" + 0.296*\"公投\" + -0.237*\"退休\" + -0.218*\"改革\" + 0.207*\"課綱\" + 0.203*\".\" + -0.166*\"公教\" + -0.148*\"金\" + -0.148*\"所得\" + -0.122*\"％\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(corpus_lsi)\n",
    "index.num_best = MAXCAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(q):\n",
    "    q = taskseg(q)\n",
    "    vec_bow = dictionary.doc2bow(q)\n",
    "    vec_lsi = lsi[vec_bow]    \n",
    "    sims = index[vec_lsi]\n",
    "#     print(sims)\n",
    "    return [(index2doc[i],s) for i,s in sims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query,News_Index,Relevance\n",
      "\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "if not test:\n",
    "    num_train = 20\n",
    "    train_scores = {}\n",
    "    train_qlist = []\n",
    "    with open(training_name, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                print(line)\n",
    "            else:\n",
    "                fields = line.strip().split(',') # q_id, text\n",
    "                q, d, score = fields\n",
    "\n",
    "                score = int(score)\n",
    "\n",
    "                if q not in train_scores:\n",
    "                    train_scores[q] = {d:score}                \n",
    "                    train_qlist.append(q)\n",
    "                else:\n",
    "                    train_scores[q][d] = score \n",
    "    # print(\"done\")\n",
    "    train_qlist = train_qlist[:num_train]\n",
    "    print(len(train_qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4332192861775118\n",
      "0.6416222623149652\n",
      "0.5233603044544168\n",
      "0.42783989871103506\n",
      "0.5123652579068676\n",
      "0.4612807963030497\n",
      "0.5973245331901924\n",
      "0.6983666654794483\n",
      "0.6440772777154382\n",
      "0.8418262178323148\n",
      "0.0\n",
      "0.5684266210012836\n",
      "0.5395295470163508\n",
      "0.8048895774057374\n",
      "0.6902311423456654\n",
      "0.5949593105455783\n",
      "0.5442795859280566\n",
      "0.8338370815572085\n",
      "0.7613996045205241\n",
      "0.3271313125607261\n",
      "[NDCG] 0.5722983141483184\n"
     ]
    }
   ],
   "source": [
    "if not test:\n",
    "    mean = 0\n",
    "\n",
    "    for qtext in train_qlist:        \n",
    "        scores = match(qtext)\n",
    "\n",
    "        myrank = [train_scores[qtext][d] if d in train_scores[qtext] else 0 for d, s in scores]\n",
    "        perfrank = sorted(myrank)[::-1]\n",
    "\n",
    "        my_dcg = 0\n",
    "        for i,r in enumerate(myrank):\n",
    "            if i == 0:\n",
    "                my_dcg += r\n",
    "            else:\n",
    "                my_dcg += r/math.log(i+1, 2)\n",
    "\n",
    "        perf_dcg = 0\n",
    "        for i,r in enumerate(perfrank):\n",
    "            if i == 0:\n",
    "                perf_dcg += r\n",
    "            else:\n",
    "                perf_dcg += r/math.log(i+1, 2)\n",
    "\n",
    "        cur = my_dcg/(perf_dcg+1e-8)\n",
    "        print(cur)\n",
    "        mean += cur\n",
    "\n",
    "    print(\"[NDCG]\", mean / len(train_qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query_Index,Query\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load queries\n",
    "if test:\n",
    "    qlist = []\n",
    "    qids = {}\n",
    "    with open(query_name, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                print(line)\n",
    "            else:\n",
    "                fields = line.strip().split(',') # q_id, text\n",
    "                qlist.append(fields[1])\n",
    "                qids[fields[1]] = fields[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "Query 2\n",
      "Query 3\n",
      "Query 4\n",
      "Query 5\n",
      "Query 6\n",
      "Query 7\n",
      "Query 8\n",
      "Query 9\n",
      "Query 10\n",
      "Query 11\n",
      "Query 12\n",
      "Query 13\n",
      "Query 14\n",
      "Query 15\n",
      "Query 16\n",
      "Query 17\n",
      "Query 18\n",
      "Query 19\n",
      "Query 20\n"
     ]
    }
   ],
   "source": [
    "if test:\n",
    "    with open(outcsv_name, 'w') as writer:\n",
    "        writer.write(\"Query_Index\")\n",
    "        for i in range(MAXCAND):\n",
    "            writer.write(\",Rank_{:03d}\".format(i+1))\n",
    "   \n",
    "        for j,qtext in enumerate(qlist):\n",
    "            print(\"Query {}\".format(j+1))\n",
    "            \n",
    "            scores = match(qtext)\n",
    "\n",
    "            writer.write('\\n'+qids[qtext])\n",
    "            for doc_id, s in scores:\n",
    "                writer.write(','+doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
